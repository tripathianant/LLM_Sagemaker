{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cda54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kiit\\anaconda3\\lib\\site-packages (4.39.2)\n",
      "Requirement already satisfied: datasets[s3]==2.18.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: sagemaker>=2.190.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (2.214.1)\n",
      "Requirement already satisfied: huggingface_hub[cli] in c:\\users\\kiit\\anaconda3\\lib\\site-packages (0.22.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (3.8.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (6.0)\n",
      "Requirement already satisfied: s3fs in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from datasets[s3]==2.18.0) (0.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (1.34.74)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (4.25.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (6.0.0)\n",
      "Requirement already satisfied: pathos in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (0.3.2)\n",
      "Requirement already satisfied: schema in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (3.10.0)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (1.7.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (1.26.16)\n",
      "Requirement already satisfied: docker in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (7.0.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from sagemaker>=2.190.0) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from huggingface_hub[cli]) (4.7.1)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.36)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.74 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.190.0) (1.34.74)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.190.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.190.0) (0.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from aiohttp->datasets[s3]==2.18.0) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from aiohttp->datasets[s3]==2.18.0) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from aiohttp->datasets[s3]==2.18.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from aiohttp->datasets[s3]==2.18.0) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from aiohttp->datasets[s3]==2.18.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from aiohttp->datasets[s3]==2.18.0) (1.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.190.0) (3.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets[s3]==2.18.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets[s3]==2.18.0) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets[s3]==2.18.0) (0.4.6)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from docker->sagemaker>=2.190.0) (305.1)\n",
      "Requirement already satisfied: six in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from google-pasta->sagemaker>=2.190.0) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from jsonschema->sagemaker>=2.190.0) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pandas->datasets[s3]==2.18.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pandas->datasets[s3]==2.18.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pandas->datasets[s3]==2.18.0) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pathos->sagemaker>=2.190.0) (1.7.6.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pathos->sagemaker>=2.190.0) (0.3.4)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from schema->sagemaker>=2.190.0) (21.6.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers \"datasets[s3]==2.18.0\" \"sagemaker>=2.190.0\" \"huggingface_hub[cli]\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e645aacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\KIIT\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730cde13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3933f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='SageMakerTest')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63458ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(12500))\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "dataset = dataset.train_test_split(test_size=2500/12500)\n",
    "\n",
    "print(dataset[\"train\"][345][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_path = f's3://{sess.default_bucket()}/datasets/text-to-sql'\n",
    "\n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57843b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/train_dataset.json', # path where sagemaker will save training dataset\n",
    "  'model_id': \"codellama/CodeLlama-7b-hf\",           # or `mistralai/Mistral-7B-v0.1`\n",
    "  'max_seq_len': 3072,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 3,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch_fused\",                      # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'learning_rate': 2e-4,                             # learning rate, based on QLoRA paper\n",
    "  'bf16': False,                                      # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'codellama-7b-hf-text-to-sql-exp1'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_sft.py',    # train script\n",
    "    source_dir           = 'llm-sagemaker-sample/scripts/trl/',#'https://github.com/philschmid/llm-sagemaker-sample/blob/main/scripts/trl',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.t3.medium',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    environment          = {\n",
    "                            \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "                             \"HF_TOKEN\": \"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\" # huggingface token to access gated models, e.g. llama 2\n",
    "                            },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03906571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.4.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c00667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to give SageMaker the time to download the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "\n",
    "# Load the test dataset from s3\n",
    "S3Downloader.download(f\"{training_input_path}/test_dataset.json\", \".\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\",split=\"train\")\n",
    "random_sample = test_dataset[345]\n",
    "\n",
    "def request(sample):\n",
    "    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = llm.predict({\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"stop\": [\"<|im_end|>\"],\n",
    "      }\n",
    "    })\n",
    "    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}\n",
    "\n",
    "print(random_sample[\"messages\"][1])\n",
    "request(random_sample[\"messages\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07eaced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "\n",
    "# Load the test dataset from s3\n",
    "S3Downloader.download(f\"{training_input_path}/test_dataset.json\", \".\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\",split=\"train\")\n",
    "random_sample = test_dataset[345]\n",
    "\n",
    "def request(sample):\n",
    "    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = llm.predict({\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"stop\": [\"<|im_end|>\"],\n",
    "      }\n",
    "    })\n",
    "    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}\n",
    "\n",
    "print(random_sample[\"messages\"][1])\n",
    "request(random_sample[\"messages\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d802612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(sample):\n",
    "    predicted_answer = request(sample[\"messages\"][:2])\n",
    "    if predicted_answer[\"content\"] == sample[\"messages\"][2][\"content\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "success_rate = []\n",
    "number_of_eval_samples = 1000\n",
    "# iterate over eval dataset and predict\n",
    "for s in tqdm(test_dataset.shuffle().select(range(number_of_eval_samples))):\n",
    "    success_rate.append(evaluate(s))\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = sum(success_rate)/len(success_rate)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b84eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
